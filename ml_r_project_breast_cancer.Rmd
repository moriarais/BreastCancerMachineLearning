---
title: "ml_in_r_final_project"
author: "Shira, Moria, Naama"
date: "`r Sys.Date()`"
output: pdf_document
---

```{r setup, include=FALSE,results='hide'}
library(googledrive)
library(gmodels)
library(C50)
library(factoextra)
library(gridExtra)
library(glmnetUtils)
library(corrplot)
library(randomForest)
library("devtools")
library("factoextra")
library(ggplot2)
library(rpart)
library(e1071)
library(randomForest)
library(caret)
library(readr)
library(class)
library(RColorBrewer)
library(FNN)
library(cluster)
library(ggplot2)
library(survival)
library(survminer)
library(ggpubr)
library(dplyr)
library(httr)
```

# Machine learning project - breast cancer
# Introduction
In this project we use a data set from TCGA databace - Breast Invasive Carcinoma. Breast cancer can be categorized into several molecular subtypes based on the expression of specific genes and proteins. These subtypes have important implications for prognosis, treatment selection, and targeted therapies. therefore in this project we desided to use ml models and try to predict a patient subtype.
The subtypes:
Luminal A: Slow-growing, hormone receptor-positive (estrogen and/or progesteron).
Luminal B: Faster-growing, hormone receptor-positive.
HER2-positive: HER2 protein overexpression.
Triple-negative and Basal-like: lacks hormone receptors and HER2. 
Normal-like: Resembles normal tissue but still cancerous.
# Loading the data
1. The mutations data is a binary data with the columns as the gene names
2. we also have a clinical data matching the patients id in the mutations data and rna seq data.
3. scaled RNAseq data with 56498 columns (genes) and 979 patients. we used the scaled data so its ready to use and already normlized.
```{r}
y_mutations <- read.csv("y_mutations2.csv")
Breast_cancer <- read.delim("brca_tcga_clinical_data.tsv")
bc_data<- read.csv("Breast_cancer_staging_V2.csv")
scaled_data <- read.csv("scaled_x_rna_seq2.csv")
```
# Organize and clean the data
cleaning the data and selecting the features of interest- subtypes and different receptores. the clinical data has 115 features but we will be focusing on only a few that are related to our initial goul. also, a lot of the other columns have missing values.
```{r}
scaled_data <- na.omit(scaled_data)#clean null values.
y_mutations <- na.omit(y_mutations)#clean null values.
subtype_df <- subset(bc_data, select=c(bcr_patient_barcode,subtype_BRCA_Subtype_PAM50,breast_carcinoma_estrogen_receptor_status.x,
                                  breast_carcinoma_progesterone_receptor_status.x,lab_proc_her2_neu_immunohistochemistry_receptor_status.x))
colnames(subtype_df) <- c("id", "subtype", "estrogen_receptor","progesterone_receptor","her2_receptor")
subtype_df <- na.omit(subtype_df)
data<-merge(scaled_data, subtype_df, by.x = "X", by.y = "id")#merging the data
numeric_data<-(subset(data, select = -c(X, subtype,estrogen_receptor,progesterone_receptor,her2_receptor)))#numeric data for farther analysis
```

#survival plot
```{r,fig.show='hold',fig.width=5,fig.height=3}
tmp <- subtype_df[, c("id", "subtype")]
tmp2<-Breast_cancer[,c("Patient.ID","Overall.Survival.Status","Overall.Survival..Months.")]
tmp2$Overall.Survival.Status<- ifelse(tmp2$Overall.Survival.Status == "0:LIVING", 0, 1)

colnames(tmp2) <- c("id", "Survival_Status", "Survival_Months")
subtype_alive_data<-(merge(tmp2, tmp, by= "id"))

# Create a plot with survival curves for high and low expression
ggsurvplot(survfit(Surv(Survival_Months, Survival_Status) ~ subtype, data = subtype_alive_data), 
                data = subtype_alive_data, risk.table = FALSE, pval = TRUE,
                risk.table.y.text = FALSE, risk.table.height = 0.3,
                palette = c("#2E9FDF", "purple","yellow","lightgreen","pink"), conf.int = TRUE,
                xlab = "Time from Diagnosis (months)",
                ylab = "Proportion Survival",
                font.x = c(10, "black"),
                font.y = c(10, "black"),
                font.tickslab = c(10, "black"),
                legend = c(0.86, 0.7),
                title = "Survival vs Subtypes")
```

# PCA (Principal Component Analysis)
This analysis can help us vizualize the RNA-seq data vs the different subtypes, by reducing the dimetions (remember? we have over 50000 genes).
```{r}
pca <- prcomp(numeric_data)
```

```{r, fig.height=4,fig.width=6}
fviz_pca_ind(pca, col.ind="cos2", geom = "point") +
      scale_color_gradient2(low="white", mid="red",high="blue", midpoint=0.2)+ theme_minimal()
```

```{r,warning=FALSE,fig.show='hold',fig.height=4,fig.width=6, out.width='50%'}
p1 <- fviz_pca_ind(pca, label="none", habillage=data$subtype,
             addEllipses=TRUE, ellipse.level=0.95) + ggtitle("Subtype")
print(p1)
p2 <- fviz_pca_ind(pca, label="none", habillage=data$her2_receptor,
             addEllipses=TRUE, ellipse.level=0.95) + ggtitle("her2_receptor")
print(p2)
p3 <- fviz_pca_ind(pca, label="none", habillage=data$estrogen_receptor,
             addEllipses=TRUE, ellipse.level=0.95) + ggtitle("estrogen_receptor")
print(p3)
p4 <- fviz_pca_ind(pca, label="none", habillage=data$progesterone_receptor,
             addEllipses=TRUE, ellipse.level=0.95) + ggtitle("progesterone_receptor")
print(p4)
```
As we can see, it's easier to differentiate when it comes to estrogen or progesterone receptor, but the her2 data is not great and mostly inconclusive or undefined. her2 (basal-like) subtype is rare also in the population in general. the subtypes are presented as we expected- lumA and lumB are very close, Basal is more defined as a group than her2, and normal is somewhere in between.


#Machine Learning Models
*SVM
*decision tree
*random forest
*kmeans

in our analysis, out of 4 algorithms, we used 2 linear algorithms - Svm and Kmeans and 2 non-linear algorithms Decision-tree and Random-forest. we assume that non-linear algorithms would show better results when it comes to subtype and her2 stat which didn't show a  clear linear difference in the PCA.

## kmeans 
k-Means is an unsupervised algorithm that partitions a dataset into 'k' clusters based on similarity between data points and cluster, with the goal of minimizing the variance within each cluster. 
We wanted to explor if editing the data subtype class will male a diffrence. since between LumA and LumB there isn't a significant molecular different, we renamed them to LumA/B. we hope to observe better separation between the clusters. (the kmeans before the edited data can be found in the supplementary)
```{r}
subtype_data <- data %>%
  mutate(subtype = case_when(
    subtype %in% c("LumA", "LumB") ~ "lumA/B",
    TRUE ~ subtype
  ))

numeric_subtype_data<-(subset(subtype_data, select = -c(X, subtype,estrogen_receptor,progesterone_receptor,her2_receptor)))
```

```{r,fig.height=4,fig.width=6}
rna_matrix <- as.matrix(numeric_subtype_data)
#pca <- prcomp(rna_matrix) # we already did
set.seed(123)  # For reproducibility
k <- 3 # Number of clusters
kmeans_result <- kmeans(rna_matrix, centers = k)

# Get the cluster assignments for each data point
cluster_labels <- kmeans_result$cluster
#unique(subtype_data$subtype)
# Assuming 'Class' column is a factor
class_labels <- subtype_data$subtype
pca_data <- data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2], Cluster = factor(cluster_labels), Class = factor(class_labels))

class_colors <- c("lumA/B"  = "blue",  "her2"= "red", "Basal"="lightgreen","Normal"="black")

# Plot PCA with clusters and class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Class, shape = Cluster)) +
  geom_point() +
  labs(title = "RNA-seq Data Clustering using k-means (k = 3)") +
  scale_color_manual(values = class_colors) +
  guides(shape = guide_legend(title = "Cluster"), color = guide_legend(title = "Class"))

```

this way, lumA/B has a clearer diffrence from Basal and Her2.

##Splitting the Data into train and test
We choose to split the data into training set (80%) and testing set (20%).
we tried a few options, since some classes are rare in the data (like her2), some alternative options are in the supplementary.
```{r}
set.seed(123)
data_indices <- seq_len(nrow(numeric_data))
# Sample 75% of the rows for the training set - indices. 
train_indices <- sample(data_indices, size = 0.8 * nrow(numeric_data))
train_data <- numeric_data[train_indices, ]# Creating the training set using the sampled row indices
test_data <- numeric_data[-train_indices, ]# Creating the testing set by excluding the training set row indices
train_labels_er <- as.factor(data$estrogen_receptor[train_indices])
test_labels_er <- data$estrogen_receptor[-train_indices]
train_labels_pr <- as.factor(data$progesterone_receptor[train_indices])
test_labels_pr <- data$progesterone_receptor[-train_indices]
train_labels_her2 <- as.factor(data$her2_receptor[train_indices])
test_labels_her2 <- data$her2_receptor[-train_indices]
train_labels_subtype <- as.factor(data$subtype[train_indices])
test_labels_subtype <- data$subtype[-train_indices]
```

## Support Vector Machine (SVM)
since our data is complex, SVM model likely has the ability to regulate and help control overfitting, handling multiclass classification by using various technique, and it can also  capture interactions between genes that contribute to specific subtypes.

```{r}
# SVM function with radial basis kernel (you can try other kernels as well)
svm_model_er <- svm(x = train_data, y = train_labels_er, kernel = "radial")
svm_model_pr <- svm(x = train_data, y = train_labels_pr, kernel = "radial")
svm_model_her2 <- svm(x = train_data, y = train_labels_her2, kernel = "radial")
svm_model_subtype <- svm(x = train_data, y = train_labels_subtype, kernel = "radial")

# Make predictions on the testing set
predictions_er <- predict(svm_model_er, newdata = test_data)
predictions_pr <- predict(svm_model_pr, newdata = test_data)
predictions_her2 <- predict(svm_model_her2, newdata = test_data)
predictions_subtype <- predict(svm_model_subtype, newdata = test_data)

# Evaluate the model for each classification task (you can use accuracy, confusion matrix, etc.)
accuracy_er <- mean(predictions_er == test_labels_er)
accuracy_pr <- mean(predictions_pr == test_labels_pr)
accuracy_her2 <- mean(predictions_her2 == test_labels_her2)
accuracy_subtype <- mean(predictions_subtype == test_labels_subtype)

# Print the accuracies
print(paste("Accuracy for Estrogen Receptor:", accuracy_er))
print(paste("Accuracy for Progesterone Receptor:", accuracy_pr))
print(paste("Accuracy for HER2 Receptor:", accuracy_her2)) 
print(paste("Accuracy for Subtype:", accuracy_subtype))
```
with accuracy of "Accuracy for Subtype: 0.797520661157025" we are now trying to find the best parameters for subtype classification, to create the best model. the accuracy got up to 0.8316832.
```{r}
C_values <- c(0.01, 0.1, 1, 10, 100)

best_accuracy <- 0
best_svm_model <- NULL

for (C_val in C_values) {
  svm_model <- svm(x = train_data, y = train_labels_subtype, kernel = "radial", cost = C_val)
  predictions <- predict(svm_model, newdata = test_data)
  accuracy <- mean(predictions == test_labels_subtype)
  
  if (accuracy > best_accuracy) {
    best_accuracy <- accuracy
    best_svm_model <- svm_model
    conf_matrix <- table(Actual = test_labels_subtype, Predicted = predictions)

  }
}
print("SVM Model:")
print(best_svm_model)
print(best_accuracy)
print(conf_matrix)
```
## decision tree
The C5.0() function helps us use boosting with our C5.0 decision tree. We add a trials parameter to show how many trees to use in the boosting team. we choose 10 trials. The parameter sets a limit, stopping if more trees won't improve the accuracy.
###Estrogen 
```{r}
model <- C5.0(train_data, train_labels_er,trials = 10)# applying the model on the training data
pred <- predict(model, test_data)#predict the estrogen receptor stat based ont he test
acc <- sum(pred == test_labels_er) / length(test_labels_er)
print(paste("Accuracy:", round(acc * 100, 2), "%"))
#CrossTable(test_labels_er, pred, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE)
```
               | prediction 
actual value   |  Negative |  Positive | Row Total | 
---------------|-----------|-----------|-----------|
      Negative |        54 |         7 |        61 | 
               |     0.223 |     0.029 |           | 
---------------|-----------|-----------|-----------|
      Positive |         9 |       172 |       181 | 
               |     0.037 |     0.711 |           | 
---------------|-----------|-----------|-----------|
  Column Total |        63 |       179 |       242 | 
---------------|-----------|-----------|-----------|
"Accuracy: 93.39 %"

###Her2
```{r}
model_dt_her2 <- C5.0(train_data, train_labels_her2, model = "C5.0Rules")
pred_her2 <- predict(model_dt_her2, test_data)
acc <- sum(pred_her2 == test_labels_her2) / length(test_labels_her2)
print(paste("Accuracy:", round(acc * 100, 2), "%"))
#print(CrossTable(test_labels_her2, pred_her2, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE))
```
"Accuracy: 65.7 %". this accuracy is after a significante improvement: from accuracy of 53.31 %, after changing the model to "C5.0Rules" (the original model can be found in extras). as we can see, the accuracy still isn't high but again, the data itself is lacking, which make it harder for the algorithm to preform better.

###Progesteron
```{r}
model_dt_pr <- C5.0(train_data, train_labels_pr,trials = 10)
pred_pr <- predict(model_dt_pr, test_data)
acc <- sum(pred_pr == test_labels_pr) / length(test_labels_pr)
print(paste("Accuracy:", round(acc * 100, 2), "%"))
#print(CrossTable(test_labels_pr, pred_pr, prop.chisq = FALSE, prop.c = FALSE, prop.r = FALSE))
```

###Subtype
this is very low. no metter the trial number or the model it didnt improve. we will count on the other algorithms
```{r}
model_dt_subtype<- C5.0(train_data, train_labels_subtype,tirals=15)
pred_subtype <- predict(model_dt_subtype, test_data)
acc <- sum(pred_subtype == train_labels_subtype) / length(train_labels_subtype)
print(paste("Accuracy:", round(acc * 100, 2), "%"))
```
as we can see, the estrogen and progesteron receptors have the best accuracy so far. her2, with a small data, dosn't show the best results. and subtype- our main goul in this project show accuracy over 80%. we will go through a few more ml algorithms to try and perfect as we can the accuracy, with foucos on subtype.

## random forest
Random Forest is a machine learning method that builds multiple decision trees and combines their predictions for improved accuracy and robustness in handling diverse data patterns. It reduces overfitting and captures complex relationships by using random subsets of data and features. 
first, we created a list of numbers of trees, to try and find the best option in our analysis.

ntree_values <- c(50, 100,150, 200, 500, 800, 1000, 1200, 1500)
results <- data.frame(ntree=integer(), mtry=integer(), accuracy=numeric())

for (nt in ntree_values) {
    rf_model <- randomForest(train_data, train_labels_subtype, ntree = nt)
    rf_predictions <- predict(rf_model, test_data)
    accuracy <- sum(rf_predictions == test_labels_subtype) / length(test_labels_subtype)
    results <- rbind(results, c(nt, mt, accuracy))
}


```{r,include=FALSE}
results<- read.csv("results_randomForest_8.2.csv")
```

```{r,fig.width=6,fig.height=3}
ggplot(data = results, aes(x = N.trees, y = Accuracy)) +
  geom_line(color = "blue", size = 0.7) +
  geom_point(color = "blue", size = 2, shape = 16) +
  labs(x = "Number of Trees", y = "Accuracy", title = "Accuracy vs. Number of Trees") +
  theme_minimal()
```
as you can see, the ntree that gives as the best accuracy is 1000 treen. given our large data it makes sence that we will use a significante amount of trees in the parameters.

#Conclusion:
our clinical data, as mentioned before is significantly lacking, instead of nulls we have a lot of  "Indeterminate" or "Equivocal". when we tried to clean the data from rows containing those terms instead of positive/negative, we were left with a significantly low amount of samples. in addition, a lot of algorithms didn't work on our RNA seq data, which course us to be very limited to a few versions of libraries in r strong enough to build models and analyze large data (over 50000 genes).
 back to our results:
Random Forest and SVM showed similar results, with subtype.
Decision tree performed well with ER and PR.

for future analysis, we would reanalyze everything with different data, more conclusive data without a lot of question make as we have here in the clinical data... and maybe will test with the improved algorithm the  "Indeterminate" and "Equivocal" in this data:-)

\newpage
#extra
###svm

Previously, we divided the training-test data into 70/30. here we tried (80/20) 
```{r}
data_indices_80 <- seq_len(nrow(numeric_data))

# Sample 80% of the row indices for the training set
train_indices_80 <- sample(data_indices_80, size = 0.8 * nrow(numeric_data))

# Create the training set using the sampled row indices
train_data_80 <- numeric_data[train_indices_80, ]

# Create the testing set by excluding the training set row indices
test_data_80 <- numeric_data[-train_indices_80, ]

train_labels_subtype_80 <- as.factor(data$subtype[train_indices_80])
test_labels_subtype_80 <- data$subtype[-train_indices_80]
```

```{r,echo=TRUE}
svm_model_subtype_80 <- svm(x = train_data_80, y = train_labels_subtype_80, kernel = "radial")
predictions_subtype_80 <- predict(svm_model_subtype_80, newdata = test_data_80)
confusion_matrix_80 <- table(predictions_subtype_80, test_labels_subtype_80)
print(confusion_matrix_80)
accuracy_subtype_80 <- mean(predictions_subtype_80 == test_labels_subtype_80)
print(paste("Accuracy for Subtype (80%):", accuracy_subtype_80))
```
Metrics        |  70/30         | 80/20
-------------  |  ------------- | -------------
Accuracy       |  0.808         | 0.839    
Precision      |  0.7           | 0.6 
Recall         |  0.875         | 0.857
F1 score       |  0.777         | 0.705

Increasing the training set size to 80% led to improved accuracy, but with a decrease in precision and F1 score. The recall remained relatively consistent between the two configurations. 

The model's performance can be influenced by the proportion of training and testing data, we might have a lot of columns but if we are talking about her2/basal for example, not a lot of patients are classified positive and each random train test split will change the results. the her2 patients can not be in the train at all ect.

###dt
Her2
```{r}
model_dt_her2 <- C5.0(train_data, train_labels_her2)
pred_her2 <- predict(model_dt_her2, test_data)
acc <- sum(pred_her2 == test_labels_her2) / length(test_labels_her2)
print(paste("Accuracy before change of parameters: ", round(acc * 100, 2), "%"))
```
###kmeans
```{r}
rna_matrix <- as.matrix(numeric_data)
set.seed(123)  # For reproducibility
k <- 4 # Number of clusters
kmeans_result <- kmeans(rna_matrix, centers = k)

# Get the cluster assignments for each data point
cluster_labels <- kmeans_result$cluster
#unique(subtype_data$subtype)
# Assuming 'Class' column is a factor
class_labels <- data$subtype
pca_data <- data.frame(PC1 = pca$x[, 1], PC2 = pca$x[, 2], Cluster = factor(cluster_labels), Class = factor(class_labels))

class_colors<- c("LumA"  = "orange",  "Her2"= "red",   "LumB" = "green", "Basal" = "blue", "Normal"="black")

# Plot PCA with clusters and class labels
ggplot(pca_data, aes(x = PC1, y = PC2, color = Class, shape = Cluster)) +
  geom_point() +
  labs(title = "RNA-seq Data Clustering using k-means (k = 4)") +
  scale_color_manual(values = class_colors) +
  guides(shape = guide_legend(title = "Cluster"), color = guide_legend(title = "Class"))
```
###random forest
we also tried to slpit the data 70:30:

ntree_values <- c(50, 100,150, 200, 500, 800, 1000, 1200, 1500, 2000)

results <- data.frame(ntree=integer(), mtry=integer(), accuracy=numeric())

for (nt in ntree_values) {
    rf_model <- randomForest(train_data, train_labels_subtype, ntree = nt)
    rf_predictions <- predict(rf_model, test_data)
    accuracy <- sum(rf_predictions == test_labels_subtype) / length(test_labels_subtype)
    results <- rbind(results, c(nt, mt, accuracy))
}

```{r,include=FALSE}
results<- read.csv("results_randomForest.csv")
```

```{r,fig.width=6,fig.height=3}
ggplot(data = results, aes(x = N.trees, y = Accuracy)) +
  geom_line(color = "blue", size = 0.7) +
  geom_point(color = "blue", size = 2, shape = 16) +
  labs(x = "Number of Trees", y = "Accuracy", title = "Accuracy vs. Number of Trees - 70:30") +
  theme_minimal()
```


since our data is quite heavy, we are trying to create a smaller version for faster analysis. 
```{r}
variance_threshold <- 0.15
gene_variances <- apply(numeric_data, 2, var)
selected_genes <- names(gene_variances)[gene_variances >= variance_threshold]
rna_data <- numeric_data %>%   select(all_of(selected_genes))
dim(rna_data)
```

### pie charts
```{r,warning=FALSE}
sex_counts <- table(Breast_cancer$Sex)
myPalette <- brewer.pal(length(sex_counts), "Set3") 
pie(sex_counts , labels = names(sex_counts), border="white", col=myPalette )

survival_counts <- table(Breast_cancer$Overall.Survival.Status)
myPalette <- brewer.pal(length(survival_counts), "Set2") 
pie(survival_counts , labels = c("ALIVE","DEAD"), border="white", col=myPalette )

er <- table(Breast_cancer$ER.Status.By.IHC)
myPalette <- brewer.pal(length(er), "Set3") 
pie(er , labels = names(er), border="white", col=myPalette ,main="ER Status")

her2 <- table(Breast_cancer$IHC.HER2)
myPalette <- brewer.pal(length(her2), "Set3") 
pie(her2 , labels = names(her2), border="white", col=myPalette ,main="HER2 Status")

pr <- table(Breast_cancer$PR.status.by.ihc)
myPalette <- brewer.pal(length(pr), "Set3") 
pie(pr , labels = names(pr), border="white", col=myPalette ,main="PR Status")
```