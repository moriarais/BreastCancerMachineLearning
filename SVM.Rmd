---
title: "SVM"
author: "Shira, Moria, Naama"
date: "2023-07-24"
output: html_document
---

```{r setup, include=FALSE}
library(googledrive)
library(glmnetUtils)
library(corrplot)
library(randomForest)
library("devtools")
library("factoextra")
library(ggplot2)
library(rpart)
library(e1071)
library(randomForest)
library(caret)
library(readr)
library(class)
library(RColorBrewer)
library(FNN)
library(cluster)
library(survival)
library(survminer)
library(ggpubr)
library(dplyr)
library(httr)
```


```{r}
Breast_cancer <- read.delim("brca_tcga_clinical_data.tsv")
head(Breast_cancer)
```

brca_tcga_clinical_data.tsv
```{r}
bc_data<- read.csv("Breast_cancer_staging_V2.csv")
head(bc_data)
```
scaled_x_rna_seq2.csv
```{r}
scaled_data <- read.csv("scaled_x_rna_seq2.csv")
dim(scaled_data)
head(scaled_data)
```


```{r}
head(bc_data$breast_carcinoma_progesterone_receptor_status.x)
  
subtype_df <- subset(bc_data, select =
                       c(bcr_patient_barcode,subtype_BRCA_Subtype_PAM50,breast_carcinoma_estrogen_receptor_status.x,
                         breast_carcinoma_progesterone_receptor_status.x,lab_proc_her2_neu_immunohistochemistry_receptor_status.x))

colnames(subtype_df) <- c("id", "subtype", "estrogen_receptor","progesterone_receptor","her2_receptor")
head(subtype_df)

# Remove rows with NA values from the RNA-seq data
subtype_df <- na.omit(subtype_df)
dim(subtype_df)

subtype_smaller_df<-subset(subtype_df, subtype_df$her2_receptor != "Equivocal")   # Apply subset function
dim(subtype_smaller_df)
```

```{r}
subtype_data <- subset(subtype_df, select = c(id,subtype))
head(subtype_data)

her2_data <- subset(subtype_df, select = c(id,her2_receptor))
head(her2_data)

progesterone_data <- subset(subtype_df, select = c(id,progesterone_receptor))
head(progesterone_data)

estrogen_data <- subset(subtype_df, select = c(id,estrogen_receptor))
head(estrogen_data)

scaled_data<-na.omit(scaled_data)
     
RNAseq_data_subtype <- merge(scaled_data, subtype_data, by.x = "X", by.y = "id")

filtered_data<-merge(scaled_data,subtype_smaller_df, by.x = "X", by.y = "id")
```

## PCA

This analysis can be insightful in understanding the variation in the RNA-seq data and how it relates to the different receptor subtypes.

```{r}
pca <- prcomp(subset(RNAseq_data_estrogen, select = -c(X, estrogen_receptor)))
```


```{r,warning=FALSE}
p <- fviz_pca_ind(pca, label="none", habillage=RNAseq_data_subtype$subtype,
             addEllipses=TRUE, ellipse.level=0.95)
print(p)
```



```{r}
data<-merge(scaled_data, subtype_df, by.x = "X", by.y = "id")
numeric_data<-(subset(data, select = -c(X, subtype,estrogen_receptor,progesterone_receptor,her2_receptor)))
head(numeric_data)
```


```{r}
dim(pca$x)

pca_data <- data.frame(pca$x, RNAseq_data_subtype$subtype)

head(pca_data)
```

# Splitting Data into Training and Testing Sets


```{r}
set.seed(123)

data_indices <- seq_len(nrow(numeric_data))

# Sample 70% of the row indices for the training set
train_indices <- sample(data_indices, size = 0.7 * nrow(numeric_data))

# Create the training set using the sampled row indices
train_data <- numeric_data[train_indices, ]

# Create the testing set by excluding the training set row indices
test_data <- numeric_data[-train_indices, ]

train_labels_subtype <- as.factor(data$subtype[train_indices])
test_labels_subtype <- data$subtype[-train_indices]
```


# Support Vector Machine (SVM)
The reasons for using  SVM for the training our data:
* Our dataset is complex and the SVM has the ability to regulate and help control overfitting. 
* SVM can handle multiclass classification by using various technique.
* SVMs can capture interactions between genes that contribute to specific subtypes.
* SVMs are well-studied and understood, and there are established techniques for hyperparameter tuning and model evaluation.



```{r}
# Install and load the required package
library(e1071)

# SVM function with radial basis kernel (you can try other kernels as well)
svm_model_subtype <- svm(x = train_data, y = train_labels_subtype, kernel = "radial")

# Make predictions on the testing set
predictions_subtype <- predict(svm_model_subtype, newdata = test_data)

```

# SVM performance

```{r}

confusion_matrix <- table(predictions_subtype, test_labels_subtype)
print(confusion_matrix)

```

```{r}
accuracy_subtype <- mean(predictions_subtype == test_labels_subtype)
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Accuracy for Subtype:", accuracy_subtype))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-Score:", f1_score))
```

# Additional metrics

```{r}
library(e1071)

conf_matrix <- table(Actual = test_labels_subtype, Predicted = predictions_subtype)

average_precision <- mean(precision)
macro_avg_precision <- mean(precision, na.rm = TRUE)
micro_avg_precision <- sum(diag(conf_matrix)) / sum(conf_matrix)


print(paste("Average Precision:", average_precision))
print(paste("Macro-Averaged Precision:", macro_avg_precision))
print(paste("Micro-Averaged Precision:", micro_avg_precision))

```

# Analyze SVM results

```{r}
# Compute the confusion matrix
conf_matrix <- table(predictions_subtype, test_labels_subtype)

# Create a confusion matrix heatmap
heatmap(conf_matrix, 
        Rowv = NA, Colv = NA, 
        col = colorRampPalette(c("white", "blue"))(100),
        main = "Confusion Matrix Heatmap",
        xlab = "Predicted Labels",
        ylab = "Actual Labels",
        cexRow = 1.2, cexCol = 1.2,
        margins = c(5, 5))

# Add color legend
legend("bottomleft", legend = c("Basal", "Her2", "LumA", "LumB", "Normal"),
       fill = colorRampPalette(c("white", "blue"))(5),
       title = "Subtypes")
```

# SVM improvements 
Oreviously, we divided the training-test data into 70/30. The following new division (80/20) is in order to improve the SVM results.

```{r}

data_indices_80 <- seq_len(nrow(numeric_data))

# Sample 80% of the row indices for the training set
train_indices_80 <- sample(data_indices_80, size = 0.8 * nrow(numeric_data))

# Create the training set using the sampled row indices
train_data_80 <- numeric_data[train_indices_80, ]

# Create the testing set by excluding the training set row indices
test_data_80 <- numeric_data[-train_indices_80, ]

train_labels_subtype_80 <- as.factor(data$subtype[train_indices_80])
test_labels_subtype_80 <- data$subtype[-train_indices_80]
```

```{r}
# Install and load the required package
library(e1071)

# SVM function with radial basis kernel (you can try other kernels as well)
svm_model_subtype_80 <- svm(x = train_data_80, y = train_labels_subtype_80, kernel = "radial")

# Make predictions on the testing set
predictions_subtype_80 <- predict(svm_model_subtype_80, newdata = test_data_80)

```


```{r}

confusion_matrix_80 <- table(predictions_subtype_80, test_labels_subtype_80)
print("Confusion Matrix:")
print(confusion_matrix_80)

```

```{r}
# Evaluate the model for each classification task
accuracy_subtype_80 <- mean(predictions_subtype_80 == test_labels_subtype_80)
precision_80 <- confusion_matrix_80[2, 2] / sum(confusion_matrix_80[, 2])
recall_80 <- confusion_matrix_80[2, 2] / sum(confusion_matrix_80[2, ])
f1_score_80 <- 2 * (precision_80 * recall_80) / (precision_80 + recall_80)

print(paste("Accuracy for Subtype (80%):", accuracy_subtype_80))
print(paste("Precision:", precision_80))
print(paste("Recall:", recall_80))
print(paste("F1-Score:", f1_score_80))
```


# Additional metrics

```{r}
library(e1071)

conf_matrix_80 <- table(Actual = test_labels_subtype_80, Predicted = predictions_subtype_80)

average_precision_80 <- mean(precision_80)
macro_avg_precision_80 <- mean(precision_80, na.rm = TRUE)
micro_avg_precision_80 <- sum(diag(conf_matrix_80)) / sum(conf_matrix_80)

print(paste("Average Precision:", average_precision_80))
print(paste("Macro-Averaged Precision:", macro_avg_precision_80))
print(paste("Micro-Averaged Precision:", micro_avg_precision_80))
```

# Analyze SVM results

```{r}
# Compute the confusion matrix
conf_matrix_80 <- table(predictions_subtype_80, test_labels_subtype_80)

# Create a confusion matrix heatmap
heatmap(conf_matrix_80, 
        Rowv = NA, Colv = NA, 
        col = colorRampPalette(c("white", "blue"))(100),
        main = "Confusion Matrix Heatmap",
        xlab = "Predicted Labels",
        ylab = "Actual Labels",
        cexRow = 1.2, cexCol = 1.2,
        margins = c(5, 5))

# Add color legend
legend("bottomleft", legend = c("Basal", "Her2", "LumA", "LumB", "Normal"),
       fill = colorRampPalette(c("white", "blue"))(5),
       title = "Subtypes")
```

# Compare between the 2 svm results

Metrics        |  70/30         | 80/20
-------------  |  ------------- | -------------
Accuracy       |  0.808         | 0.839    
Precision      |  0.7           | 0.6 
Recall         |  0.875         | 0.857
F1 score       |  0.777         | 0.705

Conclusion: Increasing the training set size to 80% led to improved accuracy, but with a decrease in precision and F1 score. The recall remained relatively consistent between the two configurations. These results suggest that the model's performance can be influenced by the proportion of training and testing data, and there's a trade-off between correctly predicting positive cases and minimizing false positives.
