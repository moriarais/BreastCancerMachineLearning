---
title: "MACHINE LEARNING PROJECT - EXPLORING BRCA DATA"
author: "Shira, Moria, Naama"
date: "2023-07-24"
output:
  pdf_document: default
  html_document: default
    fig_width: 7
    fig_height: 6
    fig_caption: true
---

## MACHINE LEARNING PROJECT - EXPLORING BRCA DATA

# Introduction

```{r setup, include=FALSE}
library(googledrive)
library(glmnetUtils)
library(corrplot)
library(randomForest)
library("devtools")
library("factoextra")
library(ggplot2)
library(rpart)
library(e1071)
library(randomForest)
library(caret)
library(readr)
library(class)
library(RColorBrewer)
library(FNN)
library(cluster)
library(survival)
library(survminer)
library(ggpubr)
library(dplyr)
library(httr)
```

# Load data

```{r}
# Clinical data
Breast_cancer <- read.delim("brca_tcga_clinical_data.tsv")
head(Breast_cancer)
# Breast cancer staging data
bc_data<- read.csv("Breast_cancer_staging_V2.csv")
head(bc_data)
# Scaled RNA-seq data
scaled_data <- read.csv("scaled_x_rna_seq2.csv")
dim(scaled_data)
head(scaled_data)
```

# Organize data and clean data

```{r}
head(bc_data$breast_carcinoma_progesterone_receptor_status.x)
  
subtype_df <- subset(bc_data, select =
                       c(bcr_patient_barcode,subtype_BRCA_Subtype_PAM50,breast_carcinoma_estrogen_receptor_status.x,
                         breast_carcinoma_progesterone_receptor_status.x,lab_proc_her2_neu_immunohistochemistry_receptor_status.x))

colnames(subtype_df) <- c("id", "subtype", "estrogen_receptor","progesterone_receptor","her2_receptor")
head(subtype_df)

# Remove rows with NA values from the RNA-seq data
subtype_df <- na.omit(subtype_df)
dim(subtype_df)

subtype_smaller_df<-subset(subtype_df, subtype_df$her2_receptor != "Equivocal")   # Apply subset function
dim(subtype_smaller_df)
```

# Pie charts according to the different types of hormones related to breast cancer.

```{r}
er <- table(subtype_df$estrogen_receptor)
myPalette <- brewer.pal(length(er), "Set3") 
pie(er , labels = names(er), border="white", col=myPalette ,main="ER Status")
her2 <- table(subtype_df$her2_receptor)
myPalette <- brewer.pal(length(her2), "Set3") 
pie(her2 , labels = names(her2), border="white", col=myPalette ,main="HER2 Status")
pr <- table(subtype_df$progesterone_receptor)
myPalette <- brewer.pal(length(pr), "Set3") 
pie(pr , labels = names(pr), border="white", col=myPalette ,main="PR Status")
types <- table(subtype_df$subtype)
myPalette <- brewer.pal(length(types), "Set3") 
pie(types , labels = names(types), border="white", col=myPalette ,main="subtype")
```

# Analyze the data and investigate main features we would like to research about their relation and affect over the subtype of breast cancer.

```{r, warning=FALSE}

subtype_df <- subset(bc_data, select =
                       c(bcr_patient_barcode,subtype_BRCA_Subtype_PAM50,breast_carcinoma_estrogen_receptor_status.x,
                         breast_carcinoma_progesterone_receptor_status.x,lab_proc_her2_neu_immunohistochemistry_receptor_status.x))

colnames(subtype_df) <- c("id", "subtype", "estrogen_receptor","progesterone_receptor","her2_receptor")
head(subtype_df)


race_counts <- table(Breast_cancer$Race.Category)
myPalette <- brewer.pal(length(race_counts), "Set2") 
pie(race_counts , labels = c("NATIVE","ASIAN","AFRICAN AMERICAN", "CAUCASIAN"), border="white", col=myPalette )

sex_counts <- table(Breast_cancer$Sex)
myPalette <- brewer.pal(length(sex_counts), "Set3") 
pie(sex_counts , labels = names(sex_counts), border="white", col=myPalette )

survival_counts <- table(Breast_cancer$Overall.Survival.Status)
myPalette <- brewer.pal(length(survival_counts), "Set2") 
pie(survival_counts , labels = c("ALIVE","DEAD"), border="white", col=myPalette )

Menopause <- table(Breast_cancer$Menopause.Status)
myPalette <- brewer.pal(length(Menopause), "Set3") 
pie(Menopause , labels = c("Indeterminate","Peri: 6-12 mos","Post : >12 mos","Pre"), border="white", col=myPalette )

```

# Survival plots

```{r}
data_bc_filtered <- Breast_cancer
data_bc_filtered$Vital_status <- ifelse(data_bc_filtered$Overall.Survival.Status == "0:LIVING", 0, 1)

# Specify the correct column names for time and status in the Surv() function
fit <- survfit(Surv(Overall.Survival..Months., Vital_status) ~ Sex, data = data_bc_filtered)

# Create a new plot with survival curves for high and low expression
g <- ggsurvplot(fit, data = data_bc_filtered, risk.table = FALSE, pval = TRUE,
                risk.table.y.text = FALSE, risk.table.height = 0.3,
                palette = c("#2E9FDF", "purple"), conf.int = TRUE,
                xlab = "Time from Diagnosis (months)",
                ylab = "Proportion Survival",
                font.x = c(17, "bold", "black"),
                font.y = c(17, "bold", "black"),
                font.tickslab = c(17, "bold", "black"),
                legend = c(0.89, 0.89),
                title = "Survival\n")

# Display the survival plot
g
```

# Merge RNA-seq data with receptor subtype data

```{r}
subtype_data <- subset(subtype_df, select = c(id,subtype))
head(subtype_data)

her2_data <- subset(subtype_df, select = c(id,her2_receptor))
head(her2_data)

progesterone_data <- subset(subtype_df, select = c(id,progesterone_receptor))
head(progesterone_data)

estrogen_data <- subset(subtype_df, select = c(id,estrogen_receptor))
head(estrogen_data)

scaled_data<-na.omit(scaled_data)
     
RNAseq_data_subtype <- merge(scaled_data, subtype_data, by.x = "X", by.y = "id")

filtered_data<-merge(scaled_data,subtype_smaller_df, by.x = "X", by.y = "id")
```

## PCA (Principal Component Analysis)



```{r}
pca <- prcomp(subset(RNAseq_data_estrogen, select = -c(X, estrogen_receptor)))
```


```{r,warning=FALSE}
p <- fviz_pca_ind(pca, label="none", habillage=RNAseq_data_subtype$subtype,
             addEllipses=TRUE, ellipse.level=0.95)
print(p)
```



```{r}
data<-merge(scaled_data, subtype_df, by.x = "X", by.y = "id")
numeric_data<-(subset(data, select = -c(X, subtype,estrogen_receptor,progesterone_receptor,her2_receptor)))
head(numeric_data)
```


```{r}
dim(pca$x)

pca_data <- data.frame(pca$x, RNAseq_data_subtype$subtype)

head(pca_data)
```
## Findings

# Splitting Data into Training and Testing Sets

```{r}
set.seed(123)

data_indices <- seq_len(nrow(numeric_data))

# Sample 70% of the row indices for the training set
train_indices <- sample(data_indices, size = 0.7 * nrow(numeric_data))

# Create the training set using the sampled row indices
train_data <- numeric_data[train_indices, ]

# Create the testing set by excluding the training set row indices
test_data <- numeric_data[-train_indices, ]

train_labels_subtype <- as.factor(data$subtype[train_indices])
test_labels_subtype <- data$subtype[-train_indices]
```


```{r}
length(train_labels_subtype)
```
```{r}
length(test_labels_subtype)
```


## Machine learning process

# Support Vector Machine (SVM)
The reasons for using  SVM for the training our data:
* Our dataset is complex and the SVM has the ability to regulate and help control overfitting. 
* SVM can handle multiclass classification by using various technique.
* SVMs can capture interactions between genes that contribute to specific subtypes.
* SVMs are well-studied and understood, and there are established techniques for hyperparameter tuning and model evaluation.



```{r}
# Install and load the required package
library(e1071)

# SVM function with radial basis kernel (you can try other kernels as well)
svm_model_subtype <- svm(x = train_data, y = train_labels_subtype, kernel = "radial")

# Make predictions on the testing set
predictions_subtype <- predict(svm_model_subtype, newdata = test_data)

```

# SVM performance

```{r}

confusion_matrix <- table(predictions_subtype, test_labels_subtype)
print(confusion_matrix)

```

```{r}
accuracy_subtype <- mean(predictions_subtype == test_labels_subtype)
precision <- confusion_matrix[2, 2] / sum(confusion_matrix[, 2])
recall <- confusion_matrix[2, 2] / sum(confusion_matrix[2, ])
f1_score <- 2 * (precision * recall) / (precision + recall)

print(paste("Accuracy for Subtype:", accuracy_subtype))
print(paste("Precision:", precision))
print(paste("Recall:", recall))
print(paste("F1-Score:", f1_score))
```

# Additional metrics

```{r}
library(e1071)

conf_matrix <- table(Actual = test_labels_subtype, Predicted = predictions_subtype)

average_precision <- mean(precision)
macro_avg_precision <- mean(precision, na.rm = TRUE)
micro_avg_precision <- sum(diag(conf_matrix)) / sum(conf_matrix)


print(paste("Average Precision:", average_precision))
print(paste("Macro-Averaged Precision:", macro_avg_precision))
print(paste("Micro-Averaged Precision:", micro_avg_precision))

```

# Analyze SVM results

```{r}
# Compute the confusion matrix
conf_matrix <- table(predictions_subtype, test_labels_subtype)

# Create a confusion matrix heatmap
heatmap(conf_matrix, 
        Rowv = NA, Colv = NA, 
        col = colorRampPalette(c("white", "blue"))(100),
        main = "Confusion Matrix Heatmap",
        xlab = "Predicted Labels",
        ylab = "Actual Labels",
        cexRow = 1.2, cexCol = 1.2,
        margins = c(5, 5))

# Add color legend
legend("bottomleft", legend = c("Basal", "Her2", "LumA", "LumB", "Normal"),
       fill = colorRampPalette(c("white", "blue"))(5),
       title = "Subtypes")
```

# SVM improvements 
Previously, we divided the training-test data into 70/30. The following new division (80/20) is in order to improve the SVM results.

```{r}

data_indices_80 <- seq_len(nrow(numeric_data))

# Sample 80% of the row indices for the training set
train_indices_80 <- sample(data_indices_80, size = 0.8 * nrow(numeric_data))

# Create the training set using the sampled row indices
train_data_80 <- numeric_data[train_indices_80, ]

# Create the testing set by excluding the training set row indices
test_data_80 <- numeric_data[-train_indices_80, ]

train_labels_subtype_80 <- as.factor(data$subtype[train_indices_80])
test_labels_subtype_80 <- data$subtype[-train_indices_80]
```

```{r}
# Install and load the required package
library(e1071)

# SVM function with radial basis kernel (you can try other kernels as well)
svm_model_subtype_80 <- svm(x = train_data_80, y = train_labels_subtype_80, kernel = "radial")

# Make predictions on the testing set
predictions_subtype_80 <- predict(svm_model_subtype_80, newdata = test_data_80)

```


```{r}

confusion_matrix_80 <- table(predictions_subtype_80, test_labels_subtype_80)
print("Confusion Matrix:")
print(confusion_matrix_80)

```

```{r}
# Evaluate the model for each classification task
accuracy_subtype_80 <- mean(predictions_subtype_80 == test_labels_subtype_80)
precision_80 <- confusion_matrix_80[2, 2] / sum(confusion_matrix_80[, 2])
recall_80 <- confusion_matrix_80[2, 2] / sum(confusion_matrix_80[2, ])
f1_score_80 <- 2 * (precision_80 * recall_80) / (precision_80 + recall_80)

print(paste("Accuracy for Subtype (80%):", accuracy_subtype_80))
print(paste("Precision:", precision_80))
print(paste("Recall:", recall_80))
print(paste("F1-Score:", f1_score_80))
```


# Additional metrics

```{r}
library(e1071)

conf_matrix_80 <- table(Actual = test_labels_subtype_80, Predicted = predictions_subtype_80)

average_precision_80 <- mean(precision_80)
macro_avg_precision_80 <- mean(precision_80, na.rm = TRUE)
micro_avg_precision_80 <- sum(diag(conf_matrix_80)) / sum(conf_matrix_80)

print(paste("Average Precision:", average_precision_80))
print(paste("Macro-Averaged Precision:", macro_avg_precision_80))
print(paste("Micro-Averaged Precision:", micro_avg_precision_80))
```

# Analyze SVM results

```{r}
# Compute the confusion matrix
conf_matrix_80 <- table(predictions_subtype_80, test_labels_subtype_80)

# Create a confusion matrix heatmap
heatmap(conf_matrix_80, 
        Rowv = NA, Colv = NA, 
        col = colorRampPalette(c("white", "blue"))(100),
        main = "Confusion Matrix Heatmap",
        xlab = "Predicted Labels",
        ylab = "Actual Labels",
        cexRow = 1.2, cexCol = 1.2,
        margins = c(5, 5))

# Add color legend
legend("bottomleft", legend = c("Basal", "Her2", "LumA", "LumB", "Normal"),
       fill = colorRampPalette(c("white", "blue"))(5),
       title = "Subtypes")
```

# Compare between the 2 SVM results

Metrics        |  70/30         | 80/20
-------------  |  ------------- | -------------
Accuracy       |  0.808         | 0.839    
Precision      |  0.7           | 0.6 
Recall         |  0.875         | 0.857
F1 score       |  0.777         | 0.705

Conclusion: Increasing the training set size to 80% led to improved accuracy, but with a decrease in precision and F1 score. The recall remained relatively consistent between the two configurations. These results suggest that the model's performance can be influenced by the proportion of training and testing data, and there's a trade-off between correctly predicting positive cases and minimizing false positives.


# Random Forsest

```{r}

```



# 

## Conclusions












